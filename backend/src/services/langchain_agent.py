"""LangChain Agent æœåŠ¡

ä½¿ç”¨é€šä¹‰ API çš„ function calling åŠŸèƒ½æ„å»ºæ™ºèƒ½å¯¹è¯ agent
åŠ¨æ€åŠ è½½æ‰€æœ‰MCPå·¥å…·ï¼Œè®©LLMè‡ªå·±é€‰æ‹©ä½¿ç”¨å“ªä¸ª
"""
import json
from typing import Any, AsyncIterator, Dict, List, Optional

import dashscope
from dashscope import Generation

from src.services.mcp_client import get_mcp_client
from src.services.deep_research_client import get_deep_research_client
from src.config.settings import settings
from src.config.logging import get_logger

logger = get_logger(__name__)

# è®¾ç½® API Key
dashscope.api_key = settings.tongyi_api_key


# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ AI åŠ©æ‰‹ï¼Œèƒ½å¤Ÿå›ç­”é—®é¢˜ã€è¿›è¡Œæ·±åº¦ç ”ç©¶å¹¶ç”Ÿæˆæ•°æ®å¯è§†åŒ–å›¾è¡¨ã€‚

ä½ çš„èƒ½åŠ›ï¼š
1. å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›æœ‰ç”¨çš„ä¿¡æ¯å’Œå»ºè®®
2. å¯¹äºå¤æ‚é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨ deep_research å·¥å…·è¿›è¡Œæ·±åº¦åˆ†æå’Œå¤šæ­¥æ¨ç†
3. å¯ä»¥ç”Ÿæˆå„ç§æ•°æ®å¯è§†åŒ–å›¾è¡¨ï¼ˆæŠ˜çº¿å›¾ã€æŸ±çŠ¶å›¾ã€é¥¼å›¾ã€æ€ç»´å¯¼å›¾ã€ç»„ç»‡æ¶æ„å›¾ç­‰25+ç§å›¾è¡¨ï¼‰

å·¥å…·ä½¿ç”¨æŒ‡å—ï¼š

**deep_research å·¥å…·**ï¼š
- é€‚ç”¨åœºæ™¯ï¼šå¤æ‚é—®é¢˜åˆ†æã€å¸‚åœºè°ƒç ”ã€æŠ€æœ¯è°ƒæŸ¥ã€å­¦æœ¯ç ”ç©¶ç­‰éœ€è¦æ·±å…¥æ€è€ƒçš„ä»»åŠ¡
- ä½•æ—¶ä½¿ç”¨ï¼šå½“é—®é¢˜éœ€è¦å¤šè§’åº¦åˆ†æã€æœç´¢æœ€æ–°ä¿¡æ¯æˆ–å¤šæ­¥æ¨ç†æ—¶

**å›¾è¡¨ç”Ÿæˆå·¥å…·**ï¼š
- ä½ æœ‰25+ç§å›¾è¡¨å·¥å…·å¯ç”¨ï¼Œæ ¹æ®ç”¨æˆ·éœ€æ±‚é€‰æ‹©åˆé€‚çš„å·¥å…·
- æ€ç»´å¯¼å›¾ï¼šä½¿ç”¨ generate_mind_map
- ç»„ç»‡æ¶æ„å›¾ï¼šä½¿ç”¨ generate_organization_chart
- æµç¨‹å›¾ï¼šä½¿ç”¨ generate_flow_diagram
- é±¼éª¨å›¾ï¼šä½¿ç”¨ generate_fishbone_diagram
- æŸ±çŠ¶å›¾ï¼šä½¿ç”¨ generate_column_chart æˆ– generate_bar_chart
- æŠ˜çº¿å›¾ï¼šä½¿ç”¨ generate_line_chart
- é¥¼å›¾ï¼šä½¿ç”¨ generate_pie_chart
- ç­‰ç­‰...

**ç‰¹åˆ«æ³¨æ„**ï¼š
- å½“ç”¨æˆ·è¯´"ç”Ÿæˆæ€ç»´å¯¼å›¾"ã€"ç”»æ€ç»´å¯¼å›¾"æ—¶ï¼Œä½¿ç”¨ generate_mind_map å·¥å…·
- æ ¹æ®å·¥å…·çš„æè¿°å’Œå‚æ•°è¦æ±‚ï¼Œæ„é€ æ­£ç¡®çš„æ•°æ®æ ¼å¼
- æ€ç»´å¯¼å›¾ç­‰å±‚æ¬¡ç»“æ„å›¾è¡¨éœ€è¦æ ‘å½¢æ•°æ®ï¼š{"name": "ä¸»é¢˜", "children": [...]}
- æ™®é€šå›¾è¡¨éœ€è¦æ•°ç»„æ•°æ®ï¼š[{"x": ..., "y": ...}, ...]
- å½“å›¾è¡¨å·¥å…·è¿”å›æˆåŠŸåï¼Œå‘Šè¯‰ç”¨æˆ·å›¾è¡¨å·²ç”Ÿæˆå¹¶ç®€è¦è¯´æ˜å†…å®¹

ç°åœ¨å¼€å§‹å¯¹è¯å§ï¼
"""


async def load_mcp_tools() -> List[Dict[str, Any]]:
    """ä»MCPæœåŠ¡å™¨åŠ¨æ€åŠ è½½æ‰€æœ‰å·¥å…·å®šä¹‰

    Returns:
        List[Dict]: é€šä¹‰APIæ ¼å¼çš„å·¥å…·åˆ—è¡¨
    """
    mcp_client = get_mcp_client()
    await mcp_client.connect()

    try:
        # è·å–MCPå·¥å…·åˆ—è¡¨
        tools_response = await mcp_client.session.list_tools()

        tongyi_tools = []
        for tool in tools_response.tools:
            # å°†MCPå·¥å…·å®šä¹‰è½¬æ¢ä¸ºé€šä¹‰APIæ ¼å¼
            tongyi_tool = {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema  # MCPçš„inputSchemaå·²ç»æ˜¯JSON Schemaæ ¼å¼
                }
            }
            tongyi_tools.append(tongyi_tool)

        logger.info(f"å·²åŠ è½½ {len(tongyi_tools)} ä¸ªMCPå·¥å…·: {[t['function']['name'] for t in tongyi_tools]}")
        return tongyi_tools

    finally:
        await mcp_client.disconnect()


async def load_deep_research_tool() -> Dict[str, Any]:
    """åŠ è½½Deep Researchå·¥å…·å®šä¹‰

    Returns:
        Dict: é€šä¹‰APIæ ¼å¼çš„å·¥å…·å®šä¹‰
    """
    return {
        "type": "function",
        "function": {
            "name": "deep_research",
            "description": (
                "æ·±åº¦ç ”ç©¶å·¥å…·ã€‚ç”¨äºåˆ†æå¤æ‚é—®é¢˜ã€è¿›è¡Œå¤šæ­¥æ¨ç†å’Œç»¼åˆç ”ç©¶ã€‚"
                "å½“ç”¨æˆ·çš„é—®é¢˜éœ€è¦æ·±å…¥åˆ†æã€å¤šè§’åº¦æ€è€ƒæˆ–éœ€è¦æœç´¢æœ€æ–°ä¿¡æ¯æ—¶ä½¿ç”¨æ­¤å·¥å…·ã€‚"
                "ä¾‹å¦‚ï¼šå¸‚åœºåˆ†æã€æŠ€æœ¯è°ƒç ”ã€å­¦æœ¯é—®é¢˜ç­‰ã€‚"
            ),
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "éœ€è¦æ·±åº¦ç ”ç©¶çš„é—®é¢˜æˆ–ä¸»é¢˜"
                    },
                    "max_tokens": {
                        "type": "integer",
                        "description": "æœ€å¤§ç”Ÿæˆtokenæ•°",
                        "default": 4096
                    }
                },
                "required": ["query"]
            }
        }
    }


class LangChainAgentService:
    """LangChain Agent æœåŠ¡ç±»

    åŠ¨æ€åŠ è½½MCPå·¥å…·ï¼Œä½¿ç”¨é€šä¹‰ API çš„ function calling
    """

    def __init__(
        self,
        model_name: str = "qwen-plus",
        temperature: float = 0.7,
        streaming: bool = True,
    ):
        """åˆå§‹åŒ– Agent æœåŠ¡

        Args:
            model_name: é€šä¹‰æ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦
            streaming: æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º
        """
        self.model_name = model_name
        self.temperature = temperature
        self.streaming = streaming
        self.tongyi_tools = None  # å»¶è¿ŸåŠ è½½

        # ç”¨äºè®°å½•æœ¬æ¬¡å¯¹è¯ç”Ÿæˆçš„å›¾è¡¨ï¼ˆåœ¨ä¿å­˜æ¶ˆæ¯æ—¶ä½¿ç”¨ï¼‰
        self.generated_charts: List[Dict[str, Any]] = []

        logger.info(f"LangChain Agent åˆå§‹åŒ– - æ¨¡å‹: {model_name}")

    async def _ensure_tools_loaded(self):
        """ç¡®ä¿å·¥å…·å·²åŠ è½½"""
        if self.tongyi_tools is None:
            logger.info("æ­£åœ¨åŠ è½½å·¥å…·...")

            # åŠ è½½MCPå·¥å…·
            mcp_tools = await load_mcp_tools()

            # åŠ è½½Deep Researchå·¥å…·
            deep_research_tool = await load_deep_research_tool()

            # åˆå¹¶æ‰€æœ‰å·¥å…·
            self.tongyi_tools = mcp_tools + [deep_research_tool]

            logger.info(f"å·¥å…·åŠ è½½å®Œæˆ - æ€»æ•°: {len(self.tongyi_tools)}")

    async def chat_stream(
        self,
        message: str,
        chat_history: Optional[List[Dict[str, str]]] = None,
        session_id: Optional[str] = None,
    ) -> AsyncIterator[str]:
        """æµå¼èŠå¤©æ¥å£ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            chat_history: å¯¹è¯å†å²ï¼ˆé€šä¹‰æ ¼å¼ï¼‰
            session_id: ä¼šè¯ ID

        Yields:
            str: AI å›å¤çš„æ–‡æœ¬å—
        """
        logger.info(
            f"å¤„ç†æµå¼èŠå¤© - Session: {session_id}, "
            f"æ¶ˆæ¯: {message[:100]}..., "
            f"å†å²æ¶ˆæ¯æ•°: {len(chat_history) if chat_history else 0}"
        )

        try:
            # ç¡®ä¿å·¥å…·å·²åŠ è½½
            await self._ensure_tools_loaded()

            # æ¸…ç©ºä¹‹å‰çš„å›¾è¡¨è®°å½•
            self.generated_charts = []

            # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]

            if chat_history:
                messages.extend(chat_history)

            messages.append({"role": "user", "content": message})

            # å·¥å…·è°ƒç”¨å¾ªç¯ï¼ˆæœ€å¤š5è½®ï¼‰
            max_iterations = 5
            iteration = 0

            while iteration < max_iterations:
                iteration += 1
                logger.info(f"Agentè¿­ä»£ {iteration}/{max_iterations}")

                # è°ƒç”¨é€šä¹‰ APIï¼ˆå¸¦å·¥å…·å®šä¹‰ï¼‰
                response = Generation.call(
                    model=self.model_name,
                    messages=messages,
                    result_format="message",
                    temperature=self.temperature,
                    tools=self.tongyi_tools,
                )

                if response.status_code != 200:
                    error_msg = f"é€šä¹‰ API é”™è¯¯: {response.code} - {response.message}"
                    logger.error(error_msg)
                    yield f"\n\nâŒ APIè°ƒç”¨å¤±è´¥: {error_msg}"
                    break

                assistant_message = response.output.choices[0].message

                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨ï¼ˆé€šä¹‰SDKçš„å¯¹è±¡éœ€è¦ç”¨ hasattr + try-exceptï¼‰
                tool_calls = []
                try:
                    if hasattr(assistant_message, 'tool_calls'):
                        tool_calls = assistant_message.tool_calls or []
                except (KeyError, AttributeError):
                    # æ²¡æœ‰tool_callså±æ€§
                    tool_calls = []

                if not tool_calls:
                    # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆå›å¤
                    logger.info("LLM ç”Ÿæˆæœ€ç»ˆå›å¤ï¼ˆæ— å·¥å…·è°ƒç”¨ï¼‰")
                    content = assistant_message.content

                    if content:
                        yield content

                    break

                # æœ‰å·¥å…·è°ƒç”¨
                logger.info(f"æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")

                # å°†assistantæ¶ˆæ¯æ·»åŠ åˆ°å†å²
                messages.append({
                    "role": "assistant",
                    "content": assistant_message.content or "",
                    "tool_calls": tool_calls
                })

                # æ‰§è¡Œå·¥å…·
                for tool_call in tool_calls:
                    # é€šä¹‰APIè¿”å›çš„tool_callå¯èƒ½æ˜¯å­—å…¸æ ¼å¼
                    if isinstance(tool_call, dict):
                        function_name = tool_call["function"]["name"]
                        function_args_str = tool_call["function"]["arguments"]
                    else:
                        # å¯¹è±¡æ ¼å¼
                        function_name = tool_call.function.name
                        function_args_str = tool_call.function.arguments

                    logger.info(f"æ‰§è¡Œå·¥å…·: {function_name}")
                    logger.debug(f"å‚æ•°å­—ç¬¦ä¸² (å‰500å­—ç¬¦): {function_args_str[:500]}...")
                    logger.debug(f"å‚æ•°å­—ç¬¦ä¸² (å200å­—ç¬¦): ...{function_args_str[-200:]}")
                    logger.debug(f"å‚æ•°å­—ç¬¦ä¸²é•¿åº¦: {len(function_args_str)}")

                    try:
                        function_args = json.loads(function_args_str)
                    except json.JSONDecodeError as e:
                        logger.error(f"JSONè§£æå¤±è´¥: {e}")
                        logger.error(f"é”™è¯¯ä½ç½®: {e.pos}, é”™è¯¯è¡Œåˆ—: line {e.lineno} column {e.colno}")
                        logger.error(f"é”™è¯¯ä½ç½®é™„è¿‘çš„å†…å®¹: ...{function_args_str[max(0, e.pos-100):min(len(function_args_str), e.pos+100)]}...")

                        # ä¿å­˜å®Œæ•´å†…å®¹åˆ°æ–‡ä»¶ç”¨äºè°ƒè¯•
                        import tempfile
                        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                            f.write(function_args_str)
                            logger.error(f"å®Œæ•´å‚æ•°å·²ä¿å­˜åˆ°: {f.name}")

                        # å°è¯•ä¿®å¤ï¼šæˆªå–åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„JSONå¯¹è±¡
                        try:
                            # å°è¯•åªè§£æåˆ°é”™è¯¯ä½ç½®ä¹‹å‰çš„æœ‰æ•ˆJSON
                            decoder = json.JSONDecoder()
                            function_args, idx = decoder.raw_decode(function_args_str)
                            logger.warning(f"JSONä¿®å¤æˆåŠŸ - æˆªå–åˆ°ä½ç½® {idx}ï¼Œå‰©ä½™å†…å®¹è¢«ä¸¢å¼ƒ")
                        except Exception as fix_error:
                            logger.error(f"JSONä¿®å¤å¤±è´¥: {fix_error}")
                            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œè¿”å›é”™è¯¯
                            tool_result = f"å·¥å…·è°ƒç”¨å¤±è´¥: å‚æ•°æ ¼å¼é”™è¯¯ - {str(e)}\n\nè¯·ä½¿ç”¨æ­£ç¡®çš„JSONæ ¼å¼ã€‚"
                            messages.append({
                                "role": "tool",
                                "content": tool_result,
                                "name": function_name
                            })
                            continue

                    logger.info(f"è§£æåçš„å‚æ•°: {json.dumps(function_args, ensure_ascii=False)[:200]}...")

                    # å‘ç”¨æˆ·æ˜¾ç¤ºå·¥å…·è°ƒç”¨ä¿¡æ¯
                    if function_name == "deep_research":
                        yield f"\n\nğŸ”§ **æ­£åœ¨è°ƒç”¨å·¥å…·**: `{function_name}`\n"
                        yield f"ğŸ“Š **åˆ†æé—®é¢˜**: {function_args.get('query', '')[:100]}...\n\n"
                    elif function_name.startswith("generate_"):
                        # å›¾è¡¨å·¥å…·
                        chart_type = function_name.replace("generate_", "").replace("_", " ")
                        yield f"\n\nğŸ”§ **æ­£åœ¨è°ƒç”¨å·¥å…·**: `{function_name}`\n"
                        yield f"ğŸ“ˆ **ç”Ÿæˆå›¾è¡¨**: {chart_type}\n\n"
                    else:
                        yield f"\n\nğŸ”§ **æ­£åœ¨è°ƒç”¨å·¥å…·**: `{function_name}`\n\n"

                    try:
                        # æ‰§è¡Œå·¥å…·
                        if function_name == "deep_research":
                            # Deep Researchå·¥å…·
                            tool_result = await self._execute_deep_research(**function_args)
                        elif function_name.startswith("generate_"):
                            # MCPå›¾è¡¨å·¥å…·
                            tool_result, chart_info = await self._execute_mcp_tool(function_name, function_args)

                            # è®°å½•å›¾è¡¨ä¿¡æ¯ï¼Œåç»­ä¿å­˜æ¶ˆæ¯æ—¶ä½¿ç”¨
                            if chart_info:
                                self.generated_charts.append(chart_info)
                                logger.info(f"è®°å½•å›¾è¡¨ä¿¡æ¯: {chart_info['chart_type']}")
                        else:
                            tool_result = f"é”™è¯¯: æœªçŸ¥å·¥å…· {function_name}"
                            logger.error(tool_result)

                        logger.info(f"å·¥å…· {function_name} æ‰§è¡ŒæˆåŠŸï¼Œç»“æœé•¿åº¦: {len(str(tool_result))}")

                    except Exception as e:
                        tool_result = f"å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
                        logger.error(f"å·¥å…· {function_name} æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)

                    # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
                    messages.append({
                        "role": "tool",
                        "content": str(tool_result),
                        "name": function_name
                    })

                # ç»§ç»­å¾ªç¯ï¼Œè®©LLMæ ¹æ®å·¥å…·ç»“æœç”Ÿæˆå›å¤

            if iteration >= max_iterations:
                logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}")
                yield "\n\nâš ï¸ å·²è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ã€‚"

        except Exception as e:
            logger.error(f"æµå¼èŠå¤©å¤„ç†å¤±è´¥: {e}", exc_info=True)
            yield f"\n\nâŒ æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°äº†é”™è¯¯: {str(e)}"

    async def _execute_deep_research(self, query: str, max_tokens: int = 4096) -> str:
        """æ‰§è¡ŒDeep Researchå·¥å…·

        Args:
            query: ç ”ç©¶é—®é¢˜
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            str: ç ”ç©¶ç»“æœ
        """
        client = get_deep_research_client()
        full_response = ""
        async for chunk in client.stream_chat(query=query, session_history=[]):
            full_response += chunk
        return full_response

    async def _execute_mcp_tool(self, tool_name: str, arguments: Dict[str, Any]) -> tuple[str, Optional[Dict[str, Any]]]:
        """æ‰§è¡ŒMCPå·¥å…·

        Args:
            tool_name: MCPå·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°

        Returns:
            tuple: (ç»™LLMçš„ç®€å•æ–‡æœ¬, å›¾è¡¨ä¿¡æ¯dictæˆ–None)
        """
        mcp_client = get_mcp_client()
        await mcp_client.connect()

        try:
            # ç›´æ¥è°ƒç”¨MCPå·¥å…·
            result = await mcp_client.session.call_tool(tool_name, arguments)

            # æå–ç»“æœ
            chart_url = None
            if hasattr(result, 'content'):
                # æå–content
                content_items = result.content
                if content_items and len(content_items) > 0:
                    chart_url = content_items[0].text

            if not chart_url:
                return "å›¾è¡¨ç”Ÿæˆå¤±è´¥ï¼šå·¥å…·è¿”å›ç©ºç»“æœ", None

            # æ„é€ å›¾è¡¨ä¿¡æ¯ï¼ˆç”¨äºåç»­ä¿å­˜ï¼‰
            chart_info = {
                "type": "image",
                "url": chart_url,
                "tool": tool_name,
                "chart_type": tool_name.replace("generate_", "").replace("_", "-")
            }

            # è¿”å›ç»™LLMçš„åªæ˜¯ç®€å•ç¡®è®¤ä¿¡æ¯
            return f"å›¾è¡¨å·²æˆåŠŸç”Ÿæˆï¼å›¾è¡¨ç±»å‹ï¼š{chart_info['chart_type']}", chart_info

        finally:
            await mcp_client.disconnect()

    def get_generated_charts(self) -> List[Dict[str, Any]]:
        """è·å–æœ¬æ¬¡å¯¹è¯ç”Ÿæˆçš„æ‰€æœ‰å›¾è¡¨ä¿¡æ¯

        ä¾›å¤–éƒ¨è°ƒç”¨ï¼Œåœ¨ä¿å­˜æ¶ˆæ¯åè·å–å›¾è¡¨ä¿¡æ¯
        """
        return self.generated_charts


# å…¨å±€ agent å®ä¾‹
_agent_service: Optional[LangChainAgentService] = None


def get_agent_service(
    model_name: str = "qwen-plus",
    temperature: float = 0.7,
    streaming: bool = True,
) -> LangChainAgentService:
    """è·å–å…¨å±€ Agent æœåŠ¡å®ä¾‹

    Args:
        model_name: æ¨¡å‹åç§°
        temperature: ç”Ÿæˆæ¸©åº¦
        streaming: æ˜¯å¦æµå¼è¾“å‡º

    Returns:
        LangChainAgentService: Agent æœåŠ¡å®ä¾‹
    """
    global _agent_service
    if _agent_service is None:
        _agent_service = LangChainAgentService(
            model_name=model_name,
            temperature=temperature,
            streaming=streaming,
        )
    return _agent_service
